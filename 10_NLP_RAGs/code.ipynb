{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "# Implementation of a RAG model for PDF Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5350990-fcc5-4679-ade8-6656f0d832ae",
   "metadata": {},
   "source": [
    "### Use Case\n",
    "\n",
    "- The following use case relates to the implementation of a RAG model with a user interface for analyzing any PDF texts.\n",
    "- \n",
    "A book on the topic of market research is provided in the knowledge base as an example text (feel free to add more specific data to the knowledge base).\n",
    "\n",
    "### Libraries\n",
    "\n",
    "- The `os` library provides tools for interacting with the operating system, such as accessing and manipulating file systems, handling environment variables, and executing system-level commands.\n",
    "- The `glob` library complements this by enabling the retrieval of file and directory paths using Unix shell-style wildcards, making it particularly useful for searching and organizing files that match specific patterns.\n",
    "- The `dotenv` library, accessed through `from dotenv import load_dotenv`, facilitates secure and convenient management of environment variables by loading them from a `.env` file into the application’s environment, which is ideal for handling sensitive information like API keys or configuration settings.\n",
    "- The `gradio` library allows the creation of interactive web-based user interfaces, making it simple to showcase and interact with machine learning models or other Python functions. Gradio provides a range of components like text input, sliders, and image uploaders, making it a popular choice for building intuitive and shareable application interfaces. Together, these libraries suggest the code is likely setting up a secure, user-friendly application, potentially for showcasing or deploying a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60fcc1f-3cdc-4748-9c07-9bc38fd83e7d",
   "metadata": {},
   "source": [
    "- The `langchain.document_loaders` module, which includes `DirectoryLoader` and `TextLoader`, enables efficient loading of documents from directories or text files. \n",
    "- The `CharacterTextSplitter` from `langchain.text_splitter` is designed for breaking down documents into smaller chunks for easier processing, while the `Document` schema from `langchain.schema` standardizes document representation.\n",
    "- Language model interaction is facilitated by `OpenAIEmbeddings` and `ChatOpenAI` from `langchain_openai`, enabling the use of OpenAI's embeddings and conversational capabilities. To store and query vectorized document embeddings, the `Chroma` vector store from `langchain_chroma` is employed.\n",
    "- The code also leverages additional libraries for specific tasks. `numpy` (imported as `np`) provides support for numerical computations, while `PyPDF2` is used for reading and processing PDF files. Dimensionality reduction for visualizing high-dimensional data is achieved using the `t-SNE` algorithm from `sklearn.manifold`.\n",
    "- To visualize the reduced data in an interactive manner, `plotly.graph_objects` is utilized.\n",
    "- For conversational applications, `ConversationBufferMemory` from `langchain.memory` manages and retains the context of conversations, and `ConversationalRetrievalChain` from `langchain.chains` integrates conversational capabilities with document retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "import PyPDF2 as PyPDF2\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection and creating a vector database\n",
    "MODEL = \"gpt-4o\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c84168a-36f4-445c-9875-92ad24c0bdf3",
   "metadata": {},
   "source": [
    "Read and extract text content from a PDF file, located in the `knowledge-base` directory. It begins by importing the `PdfReader` class from the `PyPDF2` library, which facilitates the handling and processing of PDF files. An instance of `PdfReader` is created for the specified file, enabling access to its pages. An empty string `text` is initialized to accumulate the extracted content. The code then iterates over all the pages in the PDF using a `for` loop, and for each page, the `extract_text()` method is called to extract the textual content. This text is subsequently appended to the `text` string, resulting in a single continuous string containing the combined text from all pages of the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "83439bca-0dc1-45d5-8df1-814ea8040b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PDF file\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "reader = PdfReader(\"knowledge-base/marktforschung.pdf\")\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13adc8b-bc67-48cf-866c-781801d17bf8",
   "metadata": {},
   "source": [
    "The code processes text data by first encapsulating it within a `Document` object and then optionally splitting it into smaller, manageable chunks. It begins by importing the necessary components from the `langchain` library, specifically the `Document` class from `langchain.docstore.document` and the `CharacterTextSplitter` from `langchain.text_splitter`. The text is wrapped inside a `Document` object, allowing it to be structured in a way that is compatible with LangChain's document processing tools. To facilitate downstream tasks such as text analysis, retrieval, or summarization, the code initializes a `CharacterTextSplitter` instance. This splitter is configured with a `chunk_size` of 1000 characters and an overlap of 100 characters between consecutive chunks. The `split_documents()` method is then called on the splitter, with the document passed as a list. This process results in the text being divided into smaller overlapping chunks, making it suitable for use in applications like natural language processing or information retrieval where smaller, contextually coherent pieces of text are advantageous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "391e0c30-f70b-45c6-ad20-b0b721850e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LangChain's Document class to wrap the text.\n",
    "# Split the text into chunks (if needed) using LangChain's TextSplitter utilities.\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Wrap the text in a Document object\n",
    "document = Document(page_content=text)\n",
    "\n",
    "# Optionally split text into smaller chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents([document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fac12f90-f52d-4c57-b036-af00e823a6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bb8c51-fa53-487f-84ff-a0b0a4caba7f",
   "metadata": {},
   "source": [
    "# Assess the number of tokens in the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be904653-fe35-4805-b70e-a62b7a673402",
   "metadata": {},
   "source": [
    "Assessing the number of tokens in text chunks is crucial to ensure compatibility with model token limits, maintain contextual coherence, optimize performance, and control processing costs. Proper token management prevents errors from exceeding limits, preserves context with overlaps, and ensures efficient, reliable processing in NLP pipelines.\n",
    "\n",
    "Tiktoken is a tokenization library used with OpenAI's language models to efficiently convert text into tokens and vice versa. It is designed to handle tokenization in a way that aligns with OpenAI's models, enabling precise management of token limits, cost estimation, and processing efficiency. Tiktoken is optimized for performance and supports various encoding formats for different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c72677ad-36b0-4db5-b8f6-8b0742b71f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(chunks))  # Should output <class 'str'> if it's a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82852aaf-ac4b-44f3-8ca5-20c056eb0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(chunks, list):\n",
    "    # Extract text from each Document object\n",
    "    chunks = \" \".join([chunk.page_content for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8b932321-9872-4139-b113-bea987454836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the chunk: 265159\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "tokens = encoding.encode(chunks)\n",
    "\n",
    "# Count the number of tokens\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "print(f\"Number of tokens in the chunk: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34163798-82ad-46a3-b436-caf73e96c2bc",
   "metadata": {},
   "source": [
    "Example Token Limits for OpenAI Models:\n",
    "- GPT-4 (8k context): Up to 8,192 tokens\n",
    "- GPT-4 (32k context): Up to 32,768 tokens\n",
    "- GPT-3.5-turbo: Up to 4,096 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a459e3f-e6d1-46a6-991c-71cec3629db6",
   "metadata": {},
   "source": [
    "# RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41e4642-15dc-4f69-9a63-61419abcb5e2",
   "metadata": {},
   "source": [
    "The `RecursiveCharacterTextSplitter` is a component in the LangChain library designed to split large text into smaller, manageable chunks while preserving as much semantic coherence as possible. It works by recursively splitting the text at predefined delimiters in a prioritized order, such as paragraphs, sentences, or words. If a chunk exceeds the specified size limit, the splitter moves to the next level of granularity, breaking the text further until the chunks fit within the desired size. This approach ensures that the resulting chunks remain contextually meaningful and well-suited for tasks like retrieval, summarization, or processing by language models with token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c150ff34-bc09-4734-a962-1946e0933380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 115\n",
      "Marktforschung\n",
      "Datenerhebung und Datenanalyse\n",
      "8. Auflage  Henning Kreis\n",
      "Raimund Wildner\n",
      "Alfred KußMarktforschungSN Flashcards Microlearning\n",
      "Schnelles und eﬃ  zientes Lernen mit digitalen Karteika rten – \n",
      "für Arbeit oder Studium!\n",
      "Diese Möglichkeiten bieten Ihnen die SN Flashcards:\n",
      "•Jederzeit und überall auf Ihrem Smar tphone,  Tablet oder Computer  lernen\n",
      "•D en Inhalt des Buches lernen und Ihr Wissen  testen\n",
      "•Sich durch verschieden e, mit multimedialen Komponenten angereicher te \n",
      "Fragetypen motivieren lassen  und zwischen drei Lernalgorithmen \n",
      "(Langzeit gedächtnis-,  Kurzzeitgedächtnis- oder Prüfungs-Modus) wählen\n",
      "•Ihre eigenen Fragen-Sets erstellen , um Ihre Lerner fahrung zu personalisieren\n",
      "So greifen Sie auf Ihre SN Flashcards zu:\n",
      "1. Gehen Sie auf die 1. Seite des 1. Kapitels  dieses Buches und folgen Sie den \n",
      "Anweisungen in der Box, um sich für einen SN Flashcards-A ccount anzumelden \n",
      "und auf die Flashcards-Inhalte für dieses Buch zuzugreif en.\n",
      "2. Laden Sie die SN Flashcards Mobile App aus dem Apple App Store oder\n",
      "Google Play Store herunter , öﬀ nen Sie die App und folgen Sie den Anweisungen  \n",
      "in der App .\n",
      "3. Wählen Sie in der mobilen App oder der Web-App die Lernkarten für dieses Buch \n",
      "aus und beginnen Sie zu lernen !\n",
      "Sollten Sie Schwierigkeiten haben, auf die SN Flashcards zuzugreif en, schreiben \n",
      "Sie bitte eine E-Mail an customerser vice@springernatur e.com und geben Sie in der \n",
      "Betreﬀ  zeile SN Flashcard s und den Buchtitel an.Henning Kreis · Raimund Wildner · Alfred Kuß\n",
      "Marktforschung\n",
      "Datenerhebung und Datenanalyse\n",
      "8., überarbeitete AuflageHenning Kreis\n",
      "Fachbereich Wirtschaftswissenschaften  \n",
      "HWR Berlin School of Economics and Law \n",
      "Berlin, Deutschland\n",
      "Alfred Kuß\n",
      "Marketing-Department, Freie Universität Berlin \n",
      "Berlin, DeutschlandRaimund Wildner\n",
      "V orstand MIR e. V . (vormals GfK Verein)  \n",
      "und Honorarprofessor an der Universität \n",
      "Erlangen-Nürnberg \n",
      "Fürth, Deutschland\n",
      "ISBN 978-3-658-44455-6  ISBN 978-3-658-44456-3 (eBook)\n",
      "https://doi.org/10.1007/978-3-658-44456-3\n",
      "Die Deutsche Nationalbibliothek verzeichnet diese Publikation in der Deutschen Nationalbibliografie; detail-\n",
      "lierte bibliografische Daten sind im Internet über https://portal.dnb.de abrufbar.\n",
      "© Der/die Herausgeber bzw. der/die Autor(en), exklusiv lizenziert an Springer Fachmedien Wiesbaden GmbH, \n",
      "ein Teil von Springer Nature 2004, 2007, 2010, 2012, 2014, 2018, 2021, 2024\n",
      "Das Werk einschließlich aller seiner Teile ist urheberrechtlich geschützt. Jede Verwertung, die nicht \n",
      "ausdrücklich vom Urheberrechtsgesetz zugelassen ist, bedarf der vorherigen Zustimmung des Verlags. \n",
      "Das gilt insbesondere für Vervielfältigungen, Bearbeitungen, Übersetzungen, Mikroverfilmungen und die \n",
      "Einspeicherung und Verarbeitung in elektronischen Systemen.\n",
      "Die Wiedergabe von allgemein beschreibenden Bezeichnungen, Marken, Unternehmensnamen etc. in diesem \n",
      "Werk bedeutet nicht, dass diese frei durch jedermann benutzt werden dürfen. Die Berechtigung zur Benutzung \n",
      "unterliegt, auch ohne gesonderten Hinweis hierzu, den Regeln des Markenrechts. Die Rechte des jeweiligen \n",
      "Zeicheninhabers sind zu beachten.\n",
      "Der Verlag, die Autoren und die Herausgeber gehen davon aus, dass die Angaben und Informationen in \n",
      "diesem Werk zum Zeitpunkt der Veröffentlichung vollständig und korrekt sind. Weder der Verlag noch \n",
      "die Autoren oder die Herausgeber übernehmen, ausdrücklich oder implizit, Gewähr für den Inhalt des \n",
      "Werkes, etwaige Fehler oder Äußerungen. Der Verlag bleibt im Hinblick auf geografische Zuordnungen und \n",
      "Gebietsbezeichnungen in veröffentlichten Karten und Institutionsadressen neutral.\n",
      "Planung/Lektorat: Barbara Roscher \n",
      "Springer Gabler ist ein Imprint der eingetragenen Gesellschaft Springer Fachmedien Wiesbaden GmbH und ist \n",
      "ein Teil von Springer Nature. \n",
      "Die Anschrift der Gesellschaft ist: Abraham-Lincoln-Str. 46, 65189 Wiesbaden, Germany\n",
      "Wenn Sie dieses Produkt entsorgen, geben Sie das Papier bitte zum Recycling.VMit dem vorliegenden Lehrbuch soll Studierenden und interessierten Praktikern eine \n",
      "Einführung und ein Überblick zum großen und für das Marketing bedeutsamen Gebiet \n",
      "der Marktforschung gegeben werden. Der Schwerpunkt des Buchs liegt bei den zen-\n",
      "tralen und allgemein gültigen Prinzipien und Methoden der Marktforschung, nicht bei \n",
      "technischen Details. Das Buch ist hinsichtlich seines Inhalts und Umfangs so konzipiert, \n",
      "dass es eine einführende Lehrveranstaltung zur Marktforschung begleiten kann. Dane-\n",
      "ben kann es Marketing-Praktikern dazu dienen, Maßstäbe für die Leistungsfähigkeit und \n",
      "Aussagekraft von Marktforschungsuntersuchungen kennen zu lernen. Für die selbst-\n",
      "ständige Durchführung von Untersuchungen bedarf es aber in der Regel methodischer \n",
      "Detailkenntnisse und Erfahrungen, die deutlich über den Rahmen dieses Buches hinaus-\n",
      "gehen.\n",
      "Das sehr umfangreiche Gebiet der Datenanalyse mit statistischen Methoden, wozu \n",
      "ja reichhaltige Spezial-Literatur existiert, wird im vorliegenden Lehrbuch nur in seinen \n",
      "Grundzügen dargestellt. Hier werden die Grundideen und Anwendungsmöglichkeiten \n",
      "von statistischen Methoden, die in der Marktforschung gängig sind, in möglichst leicht \n",
      "verständlicher Weise skizziert. Dabei werden nur begrenzte V orkenntnisse der Statis-\n",
      "tik vorausgesetzt. Dieses ist auch dadurch begründet, dass zwar in vielen (insbesondere \n",
      "wirtschaftswissenschaftlichen) Studiengängen Grundkenntnisse der Statistik vermittelt \n",
      "werden, dass aber die dort erworbenen Kenntnisse oftmals nur begrenzt nachhaltig sind.\n",
      "Die drei Autoren haben mit der achten Auflage die kooperative und harmonische Zu-\n",
      "sammenarbeit an diesem Buch fortgesetzt. Dabei liegt die Federführung jetzt bei Hen-\n",
      "ning Kreis. Im Vergleich zur siebten Auflage ist das gesamte Buch überarbeitet und ak-\n",
      "tualisiert worden. An einigen Stellen sind Ergänzungen hinzugekommen, z. B. im Hin-\n",
      "blick auf die Analyse qualitativer Daten (thematic analysis) und relevante Entwicklungen \n",
      "im Bereich künstlicher Intelligenz. An anderen Stellen gab es auch Straffungen. In den \n",
      "laufenden Text eingefügt und von diesem etwas abgesetzt finden sich zahlreiche Bei-\n",
      "spiele und weiterführende Hinweise, die die Lektüre leichter, abwechslungsreicher und \n",
      "ergiebiger machen sollen. Außerdem wurde häufig auf spezielle Literatur verwiesen, um \n",
      "eine vertiefende Beschäftigung mit den Methoden der Marktforschung zu erleichtern.Vorwort zur 8. AuflageVI Vorwort zur 8. Auflage\n",
      "Im Buch wird immer wieder zwischen weiblichen und männlichen Formen von Be-\n",
      "griffen (z. B. „die Managerin“ oder „der Kunde“) gewechselt. Das soll andeuten, dass \n",
      "die im Buch enthaltenen Aussagen natürlich geschlechtsneutral sind.\n",
      "Die Autoren haben versucht, grundlegende Aspekte der Marktforschung in gut ver -\n",
      "ständlicher Weise auf knappem Raum darzustellen. Das ist nicht immer leicht und man-\n",
      "che Kompromisse sind unvermeidlich. V or diesem Hintergrund bleiben die Autoren für \n",
      "kritische Anmerkungen und Hinweise zur Weiterentwicklung des Lehrbuchs dankbar.\n",
      "Birgit Borstelmann und Barbara Roscher vom Verlag Springer Gabler haben auch die \n",
      "V orbereitung der 8. Aufl. wieder kompetent und konstruktiv begleitet. Diese Zusammen-\n",
      "arbeit war stets sehr angenehm. Dafür herzlichen Dank. Für verbliebene Fehler und Un-\n",
      "zulänglichkeiten tragen natürlich die Autoren die Verantwortung.\n",
      "Berlin/Fürth  \n",
      "Januar 2024Henning Kreis\n",
      "Raimund Wildner\n",
      "Alfred KußVII\n",
      "Inhaltsverzeichnis\n",
      "1 Einführung . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\n",
      "1.1 Kennzeichnung der Marktforschung  .............................  2\n",
      "1.2 Anwendungen der Marktforschung ..............................  3\n",
      "Literatur ........................................................  6\n",
      "2 Grundlagen  ....................................................  7\n",
      "2.1 Überblick  ..................................................  7\n",
      "2.2 Zwei Sichtweisen des Forschungsprozesses – Theorie und Praxis  ......  8\n",
      "2.2.1 Untersuchungsablauf in der Marktforschungspraxis ...........  8\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Example: Set the max chunk size and overlap\n",
    "chunk_size = 8000  # Adjust to fit within your model's token limit\n",
    "chunk_overlap = 200  # Overlap ensures context continuity between chunks\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Assuming your large text is in a variable called `text`\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "# Check the number of chunks created\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "\n",
    "# Example: Inspect the first chunk\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b2f2e-3db6-4e54-ba28-65cd22ed994b",
   "metadata": {},
   "source": [
    "# Convert String to Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52794b43-4256-4ba3-86e8-95fc4d5dca8d",
   "metadata": {},
   "source": [
    "The code creates a vector store by converting text chunks into `Document` objects and generating embeddings using OpenAI's embedding model. These documents and their embeddings are stored in a Chroma vector store, with persistence enabled to save the data in a specified directory (`db_name`). Finally, it prints the number of documents stored in the vector store. This setup facilitates efficient similarity search and retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ab6c52a-8274-48ac-a690-44a8c7ccd17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "# Convert strings in `chunks` into `Document` objects\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c01b5-b67e-40bb-927b-709b3894599f",
   "metadata": {},
   "source": [
    "# Create Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534c8c1-b021-41ab-b581-c1af2c7e2f7c",
   "metadata": {},
   "source": [
    "A vector store is a database designed to store, organize, and retrieve vector representations of data, typically numerical embeddings derived from text, images, or other data types. These embeddings are high-dimensional vectors that encode semantic or contextual information, enabling efficient similarity searches. Vector stores are crucial in applications like information retrieval, recommendation systems, and machine learning pipelines, where finding similar items or documents is a core task.\n",
    "\n",
    "Chroma is an open-source vector store library designed for managing embeddings and facilitating semantic search and retrieval. It is particularly well-integrated with LangChain, making it a popular choice for building NLP and AI-powered applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "78998399-ac17-4e28-b15f-0b5f51e6ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 115 documents\n"
     ]
    }
   ],
   "source": [
    "# Clear chroma\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "057868f6-51a6-4087-94d1-380145821550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 1,536 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Get one vector and find how many dimensions it has\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468860b-86a2-41df-af01-b2400cc985be",
   "metadata": {},
   "source": [
    "## Use LangChain for Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289c14ce-fcf7-4c41-a49b-cddc695062c1",
   "metadata": {},
   "source": [
    "Setting up a conversational AI system with retrieval-augmented generation (RAG) capabilities using OpenAI's language model and LangChain tools:\n",
    "\n",
    "1. **Creating the Chat Model:**\n",
    "   - `llm = ChatOpenAI(temperature=0.7, model_name=MODEL)` initializes a conversational language model using OpenAI's API with a specified temperature (controlling creativity) and a model name (e.g., GPT-4).\n",
    "\n",
    "2. **Setting up Memory:**\n",
    "   - `memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)` establishes a memory buffer to store and manage the conversation history, ensuring the chat retains context across turns. The `return_messages` flag ensures the memory returns messages in a structured format.\n",
    "\n",
    "3. **Defining the Retriever:**\n",
    "   - `retriever = vectorstore.as_retriever()` creates a retriever from the vector store, enabling semantic search over the stored document embeddings. This is used to retrieve relevant information during the conversation.\n",
    "\n",
    "4. **Creating the Conversational Retrieval Chain:**\n",
    "   - `conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)` combines the language model, memory, and retriever into a single conversational framework. This setup allows the chatbot to use the vector store for retrieving contextually relevant information, maintain conversational memory, and generate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "129c7d1e-0094-4479-9459-f9360b95f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e6eb99fb-33ec-4025-ab92-b634ede03647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcb659-13ce-47ab-8a5e-01b930494964",
   "metadata": {},
   "source": [
    "## Use Gradio as Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c3536590-85c7-4155-bd87-ae78a1467670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping in a function\n",
    "\n",
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b252d8c1-61a8-406d-b57a-8f708a62b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5435b2b9-935c-48cd-aaf3-73a837ecde49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
