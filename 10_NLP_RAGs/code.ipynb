{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "# Implementation of a RAG model for PDF Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5350990-fcc5-4679-ade8-6656f0d832ae",
   "metadata": {},
   "source": [
    "### Use Case\n",
    "\n",
    "- The following use case relates to the implementation of a RAG model with a user interface for analyzing any PDF texts.\n",
    "- \n",
    "A book on the topic of market research is provided in the knowledge base as an example text (feel free to add more specific data to the knowledge base).\n",
    "\n",
    "### Libraries\n",
    "\n",
    "- The `os` library provides tools for interacting with the operating system, such as accessing and manipulating file systems, handling environment variables, and executing system-level commands.\n",
    "- The `glob` library complements this by enabling the retrieval of file and directory paths using Unix shell-style wildcards, making it particularly useful for searching and organizing files that match specific patterns.\n",
    "- The `dotenv` library, accessed through `from dotenv import load_dotenv`, facilitates secure and convenient management of environment variables by loading them from a `.env` file into the applicationâ€™s environment, which is ideal for handling sensitive information like API keys or configuration settings.\n",
    "- The `gradio` library allows the creation of interactive web-based user interfaces, making it simple to showcase and interact with machine learning models or other Python functions. Gradio provides a range of components like text input, sliders, and image uploaders, making it a popular choice for building intuitive and shareable application interfaces. Together, these libraries suggest the code is likely setting up a secure, user-friendly application, potentially for showcasing or deploying a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60fcc1f-3cdc-4748-9c07-9bc38fd83e7d",
   "metadata": {},
   "source": [
    "- The `langchain.document_loaders` module, which includes `DirectoryLoader` and `TextLoader`, enables efficient loading of documents from directories or text files. \n",
    "- The `CharacterTextSplitter` from `langchain.text_splitter` is designed for breaking down documents into smaller chunks for easier processing, while the `Document` schema from `langchain.schema` standardizes document representation.\n",
    "- Language model interaction is facilitated by `OpenAIEmbeddings` and `ChatOpenAI` from `langchain_openai`, enabling the use of OpenAI's embeddings and conversational capabilities. To store and query vectorized document embeddings, the `Chroma` vector store from `langchain_chroma` is employed.\n",
    "- The code also leverages additional libraries for specific tasks. `numpy` (imported as `np`) provides support for numerical computations, while `PyPDF2` is used for reading and processing PDF files. Dimensionality reduction for visualizing high-dimensional data is achieved using the `t-SNE` algorithm from `sklearn.manifold`.\n",
    "- To visualize the reduced data in an interactive manner, `plotly.graph_objects` is utilized.\n",
    "- For conversational applications, `ConversationBufferMemory` from `langchain.memory` manages and retains the context of conversations, and `ConversationalRetrievalChain` from `langchain.chains` integrates conversational capabilities with document retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "import PyPDF2 as PyPDF2\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection and creating a vector database\n",
    "MODEL = \"gpt-4o\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c84168a-36f4-445c-9875-92ad24c0bdf3",
   "metadata": {},
   "source": [
    "Read and extract text content from a PDF file, located in the `knowledge-base` directory. It begins by importing the `PdfReader` class from the `PyPDF2` library, which facilitates the handling and processing of PDF files. An instance of `PdfReader` is created for the specified file, enabling access to its pages. An empty string `text` is initialized to accumulate the extracted content. The code then iterates over all the pages in the PDF using a `for` loop, and for each page, the `extract_text()` method is called to extract the textual content. This text is subsequently appended to the `text` string, resulting in a single continuous string containing the combined text from all pages of the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "83439bca-0dc1-45d5-8df1-814ea8040b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PDF file\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "reader = PdfReader(\"knowledge-base/marktforschung.pdf\")\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13adc8b-bc67-48cf-866c-781801d17bf8",
   "metadata": {},
   "source": [
    "The code processes text data by first encapsulating it within a `Document` object and then optionally splitting it into smaller, manageable chunks. It begins by importing the necessary components from the `langchain` library, specifically the `Document` class from `langchain.docstore.document` and the `CharacterTextSplitter` from `langchain.text_splitter`. The text is wrapped inside a `Document` object, allowing it to be structured in a way that is compatible with LangChain's document processing tools. To facilitate downstream tasks such as text analysis, retrieval, or summarization, the code initializes a `CharacterTextSplitter` instance. This splitter is configured with a `chunk_size` of 1000 characters and an overlap of 100 characters between consecutive chunks. The `split_documents()` method is then called on the splitter, with the document passed as a list. This process results in the text being divided into smaller overlapping chunks, making it suitable for use in applications like natural language processing or information retrieval where smaller, contextually coherent pieces of text are advantageous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "391e0c30-f70b-45c6-ad20-b0b721850e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LangChain's Document class to wrap the text.\n",
    "# Split the text into chunks (if needed) using LangChain's TextSplitter utilities.\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Wrap the text in a Document object\n",
    "document = Document(page_content=text)\n",
    "\n",
    "# Optionally split text into smaller chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents([document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fac12f90-f52d-4c57-b036-af00e823a6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bb8c51-fa53-487f-84ff-a0b0a4caba7f",
   "metadata": {},
   "source": [
    "# Assess the number of tokens in the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be904653-fe35-4805-b70e-a62b7a673402",
   "metadata": {},
   "source": [
    "Assessing the number of tokens in text chunks is crucial to ensure compatibility with model token limits, maintain contextual coherence, optimize performance, and control processing costs. Proper token management prevents errors from exceeding limits, preserves context with overlaps, and ensures efficient, reliable processing in NLP pipelines.\n",
    "\n",
    "Tiktoken is a tokenization library used with OpenAI's language models to efficiently convert text into tokens and vice versa. It is designed to handle tokenization in a way that aligns with OpenAI's models, enabling precise management of token limits, cost estimation, and processing efficiency. Tiktoken is optimized for performance and supports various encoding formats for different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c72677ad-36b0-4db5-b8f6-8b0742b71f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(chunks))  # Should output <class 'str'> if it's a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82852aaf-ac4b-44f3-8ca5-20c056eb0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(chunks, list):\n",
    "    # Extract text from each Document object\n",
    "    chunks = \" \".join([chunk.page_content for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8b932321-9872-4139-b113-bea987454836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the chunk: 265159\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "tokens = encoding.encode(chunks)\n",
    "\n",
    "# Count the number of tokens\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "print(f\"Number of tokens in the chunk: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34163798-82ad-46a3-b436-caf73e96c2bc",
   "metadata": {},
   "source": [
    "Example Token Limits for OpenAI Models:\n",
    "- GPT-4 (8k context): Up to 8,192 tokens\n",
    "- GPT-4 (32k context): Up to 32,768 tokens\n",
    "- GPT-3.5-turbo: Up to 4,096 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a459e3f-e6d1-46a6-991c-71cec3629db6",
   "metadata": {},
   "source": [
    "# RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41e4642-15dc-4f69-9a63-61419abcb5e2",
   "metadata": {},
   "source": [
    "The `RecursiveCharacterTextSplitter` is a component in the LangChain library designed to split large text into smaller, manageable chunks while preserving as much semantic coherence as possible. It works by recursively splitting the text at predefined delimiters in a prioritized order, such as paragraphs, sentences, or words. If a chunk exceeds the specified size limit, the splitter moves to the next level of granularity, breaking the text further until the chunks fit within the desired size. This approach ensures that the resulting chunks remain contextually meaningful and well-suited for tasks like retrieval, summarization, or processing by language models with token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c150ff34-bc09-4734-a962-1946e0933380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 115\n",
      "Marktforschung\n",
      "Datenerhebung und Datenanalyse\n",
      "8. Auflage  Henning Kreis\n",
      "Raimund Wildner\n",
      "Alfred KuÃŸMarktforschungSN Flashcards Microlearning\n",
      "Schnelles und eï¬ƒ  zientes Lernen mit digitalen Karteika rten â€“ \n",
      "fÃ¼r Arbeit oder Studium!\n",
      "Diese MÃ¶glichkeiten bieten Ihnen die SN Flashcards:\n",
      "â€¢Jederzeit und Ã¼berall auf Ihrem Smar tphone,  Tablet oder Computer  lernen\n",
      "â€¢D en Inhalt des Buches lernen und Ihr Wissen  testen\n",
      "â€¢Sich durch verschieden e, mit multimedialen Komponenten angereicher te \n",
      "Fragetypen motivieren lassen  und zwischen drei Lernalgorithmen \n",
      "(Langzeit gedÃ¤chtnis-,  KurzzeitgedÃ¤chtnis- oder PrÃ¼fungs-Modus) wÃ¤hlen\n",
      "â€¢Ihre eigenen Fragen-Sets erstellen , um Ihre Lerner fahrung zu personalisieren\n",
      "So greifen Sie auf Ihre SN Flashcards zu:\n",
      "1. Gehen Sie auf die 1. Seite des 1. Kapitels  dieses Buches und folgen Sie den \n",
      "Anweisungen in der Box, um sich fÃ¼r einen SN Flashcards-A ccount anzumelden \n",
      "und auf die Flashcards-Inhalte fÃ¼r dieses Buch zuzugreif en.\n",
      "2. Laden Sie die SN Flashcards Mobile App aus dem Apple App Store oder\n",
      "Google Play Store herunter , Ã¶ï¬€ nen Sie die App und folgen Sie den Anweisungen  \n",
      "in der App .\n",
      "3. WÃ¤hlen Sie in der mobilen App oder der Web-App die Lernkarten fÃ¼r dieses Buch \n",
      "aus und beginnen Sie zu lernen !\n",
      "Sollten Sie Schwierigkeiten haben, auf die SN Flashcards zuzugreif en, schreiben \n",
      "Sie bitte eine E-Mail an customerser vice@springernatur e.com und geben Sie in der \n",
      "Betreï¬€  zeile SN Flashcard s und den Buchtitel an.Henning Kreis Â· Raimund Wildner Â· Alfred KuÃŸ\n",
      "Marktforschung\n",
      "Datenerhebung und Datenanalyse\n",
      "8., Ã¼berarbeitete AuflageHenning Kreis\n",
      "Fachbereich Wirtschaftswissenschaften  \n",
      "HWR Berlin School of Economics and Law \n",
      "Berlin, Deutschland\n",
      "Alfred KuÃŸ\n",
      "Marketing-Department, Freie UniversitÃ¤t Berlin \n",
      "Berlin, DeutschlandRaimund Wildner\n",
      "V orstand MIR e. V . (vormals GfK Verein)  \n",
      "und Honorarprofessor an der UniversitÃ¤t \n",
      "Erlangen-NÃ¼rnberg \n",
      "FÃ¼rth, Deutschland\n",
      "ISBN 978-3-658-44455-6  ISBN 978-3-658-44456-3 (eBook)\n",
      "https://doi.org/10.1007/978-3-658-44456-3\n",
      "Die Deutsche Nationalbibliothek verzeichnet diese Publikation in der Deutschen Nationalbibliografie; detail-\n",
      "lierte bibliografische Daten sind im Internet Ã¼ber https://portal.dnb.de abrufbar.\n",
      "Â© Der/die Herausgeber bzw. der/die Autor(en), exklusiv lizenziert an Springer Fachmedien Wiesbaden GmbH, \n",
      "ein Teil von Springer Nature 2004, 2007, 2010, 2012, 2014, 2018, 2021, 2024\n",
      "Das Werk einschlieÃŸlich aller seiner Teile ist urheberrechtlich geschÃ¼tzt. Jede Verwertung, die nicht \n",
      "ausdrÃ¼cklich vom Urheberrechtsgesetz zugelassen ist, bedarf der vorherigen Zustimmung des Verlags. \n",
      "Das gilt insbesondere fÃ¼r VervielfÃ¤ltigungen, Bearbeitungen, Ãœbersetzungen, Mikroverfilmungen und die \n",
      "Einspeicherung und Verarbeitung in elektronischen Systemen.\n",
      "Die Wiedergabe von allgemein beschreibenden Bezeichnungen, Marken, Unternehmensnamen etc. in diesem \n",
      "Werk bedeutet nicht, dass diese frei durch jedermann benutzt werden dÃ¼rfen. Die Berechtigung zur Benutzung \n",
      "unterliegt, auch ohne gesonderten Hinweis hierzu, den Regeln des Markenrechts. Die Rechte des jeweiligen \n",
      "Zeicheninhabers sind zu beachten.\n",
      "Der Verlag, die Autoren und die Herausgeber gehen davon aus, dass die Angaben und Informationen in \n",
      "diesem Werk zum Zeitpunkt der VerÃ¶ffentlichung vollstÃ¤ndig und korrekt sind. Weder der Verlag noch \n",
      "die Autoren oder die Herausgeber Ã¼bernehmen, ausdrÃ¼cklich oder implizit, GewÃ¤hr fÃ¼r den Inhalt des \n",
      "Werkes, etwaige Fehler oder Ã„uÃŸerungen. Der Verlag bleibt im Hinblick auf geografische Zuordnungen und \n",
      "Gebietsbezeichnungen in verÃ¶ffentlichten Karten und Institutionsadressen neutral.\n",
      "Planung/Lektorat: Barbara Roscher \n",
      "Springer Gabler ist ein Imprint der eingetragenen Gesellschaft Springer Fachmedien Wiesbaden GmbH und ist \n",
      "ein Teil von Springer Nature. \n",
      "Die Anschrift der Gesellschaft ist: Abraham-Lincoln-Str. 46, 65189 Wiesbaden, Germany\n",
      "Wenn Sie dieses Produkt entsorgen, geben Sie das Papier bitte zum Recycling.VMit dem vorliegenden Lehrbuch soll Studierenden und interessierten Praktikern eine \n",
      "EinfÃ¼hrung und ein Ãœberblick zum groÃŸen und fÃ¼r das Marketing bedeutsamen Gebiet \n",
      "der Marktforschung gegeben werden. Der Schwerpunkt des Buchs liegt bei den zen-\n",
      "tralen und allgemein gÃ¼ltigen Prinzipien und Methoden der Marktforschung, nicht bei \n",
      "technischen Details. Das Buch ist hinsichtlich seines Inhalts und Umfangs so konzipiert, \n",
      "dass es eine einfÃ¼hrende Lehrveranstaltung zur Marktforschung begleiten kann. Dane-\n",
      "ben kann es Marketing-Praktikern dazu dienen, MaÃŸstÃ¤be fÃ¼r die LeistungsfÃ¤higkeit und \n",
      "Aussagekraft von Marktforschungsuntersuchungen kennen zu lernen. FÃ¼r die selbst-\n",
      "stÃ¤ndige DurchfÃ¼hrung von Untersuchungen bedarf es aber in der Regel methodischer \n",
      "Detailkenntnisse und Erfahrungen, die deutlich Ã¼ber den Rahmen dieses Buches hinaus-\n",
      "gehen.\n",
      "Das sehr umfangreiche Gebiet der Datenanalyse mit statistischen Methoden, wozu \n",
      "ja reichhaltige Spezial-Literatur existiert, wird im vorliegenden Lehrbuch nur in seinen \n",
      "GrundzÃ¼gen dargestellt. Hier werden die Grundideen und AnwendungsmÃ¶glichkeiten \n",
      "von statistischen Methoden, die in der Marktforschung gÃ¤ngig sind, in mÃ¶glichst leicht \n",
      "verstÃ¤ndlicher Weise skizziert. Dabei werden nur begrenzte V orkenntnisse der Statis-\n",
      "tik vorausgesetzt. Dieses ist auch dadurch begrÃ¼ndet, dass zwar in vielen (insbesondere \n",
      "wirtschaftswissenschaftlichen) StudiengÃ¤ngen Grundkenntnisse der Statistik vermittelt \n",
      "werden, dass aber die dort erworbenen Kenntnisse oftmals nur begrenzt nachhaltig sind.\n",
      "Die drei Autoren haben mit der achten Auflage die kooperative und harmonische Zu-\n",
      "sammenarbeit an diesem Buch fortgesetzt. Dabei liegt die FederfÃ¼hrung jetzt bei Hen-\n",
      "ning Kreis. Im Vergleich zur siebten Auflage ist das gesamte Buch Ã¼berarbeitet und ak-\n",
      "tualisiert worden. An einigen Stellen sind ErgÃ¤nzungen hinzugekommen, z. B. im Hin-\n",
      "blick auf die Analyse qualitativer Daten (thematic analysis) und relevante Entwicklungen \n",
      "im Bereich kÃ¼nstlicher Intelligenz. An anderen Stellen gab es auch Straffungen. In den \n",
      "laufenden Text eingefÃ¼gt und von diesem etwas abgesetzt finden sich zahlreiche Bei-\n",
      "spiele und weiterfÃ¼hrende Hinweise, die die LektÃ¼re leichter, abwechslungsreicher und \n",
      "ergiebiger machen sollen. AuÃŸerdem wurde hÃ¤ufig auf spezielle Literatur verwiesen, um \n",
      "eine vertiefende BeschÃ¤ftigung mit den Methoden der Marktforschung zu erleichtern.Vorwort zur 8. AuflageVI Vorwort zur 8. Auflage\n",
      "Im Buch wird immer wieder zwischen weiblichen und mÃ¤nnlichen Formen von Be-\n",
      "griffen (z. B. â€ždie Managerinâ€œ oder â€žder Kundeâ€œ) gewechselt. Das soll andeuten, dass \n",
      "die im Buch enthaltenen Aussagen natÃ¼rlich geschlechtsneutral sind.\n",
      "Die Autoren haben versucht, grundlegende Aspekte der Marktforschung in gut ver -\n",
      "stÃ¤ndlicher Weise auf knappem Raum darzustellen. Das ist nicht immer leicht und man-\n",
      "che Kompromisse sind unvermeidlich. V or diesem Hintergrund bleiben die Autoren fÃ¼r \n",
      "kritische Anmerkungen und Hinweise zur Weiterentwicklung des Lehrbuchs dankbar.\n",
      "Birgit Borstelmann und Barbara Roscher vom Verlag Springer Gabler haben auch die \n",
      "V orbereitung der 8. Aufl. wieder kompetent und konstruktiv begleitet. Diese Zusammen-\n",
      "arbeit war stets sehr angenehm. DafÃ¼r herzlichen Dank. FÃ¼r verbliebene Fehler und Un-\n",
      "zulÃ¤nglichkeiten tragen natÃ¼rlich die Autoren die Verantwortung.\n",
      "Berlin/FÃ¼rth  \n",
      "Januar 2024Henning Kreis\n",
      "Raimund Wildner\n",
      "Alfred KuÃŸVII\n",
      "Inhaltsverzeichnis\n",
      "1 EinfÃ¼hrung . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\n",
      "1.1 Kennzeichnung der Marktforschung  .............................  2\n",
      "1.2 Anwendungen der Marktforschung ..............................  3\n",
      "Literatur ........................................................  6\n",
      "2 Grundlagen  ....................................................  7\n",
      "2.1 Ãœberblick  ..................................................  7\n",
      "2.2 Zwei Sichtweisen des Forschungsprozesses â€“ Theorie und Praxis  ......  8\n",
      "2.2.1 Untersuchungsablauf in der Marktforschungspraxis ...........  8\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Example: Set the max chunk size and overlap\n",
    "chunk_size = 8000  # Adjust to fit within your model's token limit\n",
    "chunk_overlap = 200  # Overlap ensures context continuity between chunks\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Assuming your large text is in a variable called `text`\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "# Check the number of chunks created\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "\n",
    "# Example: Inspect the first chunk\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b2f2e-3db6-4e54-ba28-65cd22ed994b",
   "metadata": {},
   "source": [
    "# Convert String to Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52794b43-4256-4ba3-86e8-95fc4d5dca8d",
   "metadata": {},
   "source": [
    "The code creates a vector store by converting text chunks into `Document` objects and generating embeddings using OpenAI's embedding model. These documents and their embeddings are stored in a Chroma vector store, with persistence enabled to save the data in a specified directory (`db_name`). Finally, it prints the number of documents stored in the vector store. This setup facilitates efficient similarity search and retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ab6c52a-8274-48ac-a690-44a8c7ccd17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "# Convert strings in `chunks` into `Document` objects\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c01b5-b67e-40bb-927b-709b3894599f",
   "metadata": {},
   "source": [
    "# Create Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534c8c1-b021-41ab-b581-c1af2c7e2f7c",
   "metadata": {},
   "source": [
    "A vector store is a database designed to store, organize, and retrieve vector representations of data, typically numerical embeddings derived from text, images, or other data types. These embeddings are high-dimensional vectors that encode semantic or contextual information, enabling efficient similarity searches. Vector stores are crucial in applications like information retrieval, recommendation systems, and machine learning pipelines, where finding similar items or documents is a core task.\n",
    "\n",
    "Chroma is an open-source vector store library designed for managing embeddings and facilitating semantic search and retrieval. It is particularly well-integrated with LangChain, making it a popular choice for building NLP and AI-powered applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "78998399-ac17-4e28-b15f-0b5f51e6ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 115 documents\n"
     ]
    }
   ],
   "source": [
    "# Clear chroma\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "057868f6-51a6-4087-94d1-380145821550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 1,536 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Get one vector and find how many dimensions it has\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468860b-86a2-41df-af01-b2400cc985be",
   "metadata": {},
   "source": [
    "## Use LangChain for Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289c14ce-fcf7-4c41-a49b-cddc695062c1",
   "metadata": {},
   "source": [
    "Setting up a conversational AI system with retrieval-augmented generation (RAG) capabilities using OpenAI's language model and LangChain tools:\n",
    "\n",
    "1. **Creating the Chat Model:**\n",
    "   - `llm = ChatOpenAI(temperature=0.7, model_name=MODEL)` initializes a conversational language model using OpenAI's API with a specified temperature (controlling creativity) and a model name (e.g., GPT-4).\n",
    "\n",
    "2. **Setting up Memory:**\n",
    "   - `memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)` establishes a memory buffer to store and manage the conversation history, ensuring the chat retains context across turns. The `return_messages` flag ensures the memory returns messages in a structured format.\n",
    "\n",
    "3. **Defining the Retriever:**\n",
    "   - `retriever = vectorstore.as_retriever()` creates a retriever from the vector store, enabling semantic search over the stored document embeddings. This is used to retrieve relevant information during the conversation.\n",
    "\n",
    "4. **Creating the Conversational Retrieval Chain:**\n",
    "   - `conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)` combines the language model, memory, and retriever into a single conversational framework. This setup allows the chatbot to use the vector store for retrieving contextually relevant information, maintain conversational memory, and generate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "129c7d1e-0094-4479-9459-f9360b95f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e6eb99fb-33ec-4025-ab92-b634ede03647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcb659-13ce-47ab-8a5e-01b930494964",
   "metadata": {},
   "source": [
    "## Use Gradio as Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c3536590-85c7-4155-bd87-ae78a1467670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping in a function\n",
    "\n",
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b252d8c1-61a8-406d-b57a-8f708a62b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5435b2b9-935c-48cd-aaf3-73a837ecde49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
